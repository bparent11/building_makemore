{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the names.txt file from github\n",
    "\n",
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier a été téléchargé avec succès.\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(\"names.txt\", \"w\") as file:\n",
    "        file.write(response.text)\n",
    "    print(\"Le fichier a été téléchargé avec succès.\")\n",
    "else:\n",
    "    print(\"Le téléchargement du fichier a échoué\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.text.splitlines() allows us to directly get the list but in the case I'm working offline, I download locally the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"names.txt\", \"r\") as file:\n",
    "#    raw_names = file.read()\n",
    "\n",
    "raw_names = open('names.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = raw_names.splitlines()\n",
    "print(len(names))\n",
    "names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the vocabulary of characters and mapping it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = sorted(list(set(''.join(name for name in names))))\n",
    "char_to_int = {char:i+1 for i, char in enumerate(characters)}\n",
    "char_to_int[\".\"] = 0 #special character to begin of put an end to a sequence\n",
    "int_to_char = {i:char for char, i in char_to_int.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 4 # number of precedents char to take into account\n",
    "\n",
    "def build_dataset(words, context_size):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for name in names:\n",
    "        context = [0] * context_size\n",
    "\n",
    "        for char in name + '.':\n",
    "            idx = char_to_int[char]\n",
    "            X.append(context)\n",
    "            y.append(idx)\n",
    "\n",
    "            #print(f\"{''.join(int_to_char[i] for i in context)}\")\n",
    "\n",
    "            context = context[1:] + [idx]\n",
    "\n",
    "    X, y = torch.tensor(X), torch.tensor(y)\n",
    "    print(f\"X.shape : {X.shape}, y.shape : {y.shape}\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape : torch.Size([228146, 4]), y.shape : torch.Size([228146])\n"
     ]
    }
   ],
   "source": [
    "X, y = build_dataset(names, context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random\n",
    "\n",
    "n1 = int(0.8 * X.shape[0])\n",
    "n2 = int(0.9 * X.shape[0])\n",
    "\n",
    "Xtr, Xval, Xtst = X.tensor_split((n1, n2), dim=0)\n",
    "ytr, yval, ytst = y.tensor_split((n1, n2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr.shape : torch.Size([182516, 4]), Xval.shape : torch.Size([22815, 4]), Xtst.shape : torch.Size([22815, 4])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Xtr.shape : {Xtr.shape}, Xval.shape : {Xval.shape}, Xtst.shape : {Xtst.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(42) # for reproductibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.rand([27, 2], generator=g) # characters to vectors\n",
    "W1 = torch.rand([4*2, 100], generator=g) # putting in the MLP 4 characters and each one of them has 2 dimensionnal vector associated\n",
    "b1 = torch.rand(100, generator=g)\n",
    "W2 = torch.rand([100, 27], generator=g)\n",
    "b2 = torch.rand(27, generator=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 4, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 8])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "print(emb.shape)\n",
    "emb.view(-1, 8).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 100])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step1 = torch.tanh(emb.view(-1, 8) @ W1 + b1)\n",
    "step1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step2 = step1 @ W2 + b2\n",
    "step2.shape # just before applying softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49.3166, 51.0630, 48.5743,  ..., 56.3222, 50.4795, 47.2554],\n",
       "        [49.0209, 50.7765, 48.2821,  ..., 56.0272, 50.1285, 46.9653],\n",
       "        [48.9999, 50.6780, 48.2724,  ..., 55.9436, 50.1138, 46.9002],\n",
       "        ...,\n",
       "        [47.5051, 49.1143, 46.9756,  ..., 54.2297, 48.5662, 45.5596],\n",
       "        [45.6855, 47.2214, 45.2956,  ..., 52.3446, 46.6939, 43.9058],\n",
       "        [47.6967, 49.4407, 47.1371,  ..., 54.5449, 48.8628, 45.8645]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(logits, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.6273)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
